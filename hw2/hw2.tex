\documentclass{article}
\usepackage{hwopt}

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Coursework (2) for \emph{Introductory Lectures on Optimization}}
\author{Xiaoyu Wang \\ 3220104364}
\date{\today}

\newcommand{\RBB}{\mathbb{R}}
\newcommand{\xB}{\mathbf{x}}
\newcommand{\yB}{\mathbf{y}}
\newcommand{\aB}{\mathbf{a}}
\newcommand{\tB}{\mathbf{t}}
\begin{document}
\maketitle

\begin{excercise}\label{e0}
For the function $f(x): \mathbb{R}^n \rightarrow \mathbb{R}^m$, please write down the zeroth-order Taylor expansion with an integral remainder term.
\end{excercise}

\begin{SOLUTION}{e0}The answer is as follows:\\
% ref:https://zhuanlan.zhihu.com/p/356937760
% ref:https://blog.csdn.net/cjm083121/article/details/97645361	
% ref:https://zhuanlan.zhihu.com/p/356937760
% ref:https://zhuanlan.zhihu.com/p/263777564
% 	we fisrt get the Taylor expansion of $f(x)$ at $a$:
% \[
% 	f(\xB)=\sum_{k=0}^n{f^{(k)}(\aB)\over k!}(\xB-\aB)^k+{(-1)^n\over n!}\int_\aB^\xB(\tB-\xB)^nf^{(n+1)}(\tB)\mathrm{d}\tB
% \]
$$
\xB-\mathbf{x_0}=(x_1-x_{01},x_2-x_{02},\cdots,x_n-x_{0n})^T
f(\xB) = {y_1,y_2,\cdots,y_m}^T
$$
when $m = 1 $,we get the zeroth-order Taylor expansion with an integral remainder term:
\[	
	f(\xB)=f(\mathbf{x_0})+\int_0^1 \left( \nabla f(\mathbf{x_0}+t(\xB-\mathbf{x_0}))\right)^{T}(\xB-\mathbf{x_0}) \mathrm{d}t
\]
when $m > 1$,we get the zeroth-order Taylor expansion with an integral remainder term:
\[
	f(\xB)=f(\mathbf{x_0})+\int_0^1  J_f(\mathbf{x_0}+t(\xB-\mathbf{x_0}))(\xB-\mathbf{x_0}) \mathrm{d}t
\] 
in the expression:\\
\[ 
J_f(t) = \begin{pmatrix} \frac{\partial f_1}{\partial t_1} & 
	\frac{\partial f_1}{\partial t_2} & \cdots & 
	\frac{\partial f_1}{\partial t_n} \\ 
	\frac{\partial f_2}{\partial t_1} & 
	\frac{\partial f_2}{\partial t_2} & 
	\cdots & \frac{\partial f_2}{\partial t_n} \\ 
	\vdots & \vdots & \ddots & \vdots \\ 
	\frac{\partial f_m}{\partial t_1} & 
	\frac{\partial f_m}{\partial t_2} & \cdots & 
	\frac{\partial f_m}{\partial t_n} \end{pmatrix} 
\]



\end{SOLUTION}

\begin{excercise}\label{e1}
Please write down the definition of the $p$-norm for a $n$-dimensional real vector.
\end{excercise}

\begin{SOLUTION}{e1}The $p$-norm of a $n$-dimensional real vector $\mathbf{x} = (x_1, x_2, \dots, x_n)$ is defined as:

\[
\|\mathbf{x}\|_p = \left( \sum_{i=1}^n |x_i|^p \right)^{1/p}
\]
	
\end{SOLUTION}


\begin{excercise}\label{e2}
Please write down the definition of the matrix norms induced by vector $p$-norms.
\end{excercise}

\begin{SOLUTION}{e2}The matrix norm induced by a vector $p$-norm is defined as:
	\[
	\|A\|_p = \sup_{\mathbf{x} \neq \mathbf{0}} \frac{\|A\mathbf{x}\|_p}{\|\mathbf{x}\|_p}
	\]
\end{SOLUTION}


\begin{excercise}\label{e3}
	Let $A$ be an $n \times n$ symmetric matrix. Proof that $A$ is positive semidefinite if and only if all eigenvalues of $A$ are nonnegative. Moreover, $A$ is positive definite if and only if all eigenvalues of $A$ are positive.
\end{excercise}

\begin{PROOF}{e3}Proof:
\begin{enumerate}
	\item Positive Semidefinite Condition:
		\begin{enumerate}
			\item If \(A\) is positive semidefinite, then all eigenvalues of \(A\) are nonnegative.
		
			\(\because\) \(A\) is positive semidefinite\\
			\(\therefore\) \(\mathbf{x}^T A \mathbf{x} \geq 0\) for any nonzero vector \(\mathbf{x} \in \mathbb{R}^n\).\\
			Let \(\lambda\) be anyone of arbitary eigenvalue of \(A\), and \(v\) (nonzero vector) be the corresponding eigenvector, that is:\\
			\[
			Av = \lambda v
			\]
			\[
			\therefore v^{T}Av = \lambda v^{T}v = \lambda \Vert v \Vert^2 \ge 0 
			\]
			\[
			\because \Vert v \Vert^2 > 0 
			\]
			\[
			\therefore \lambda \geq 0
			\]
		\item  If all eigenvalues of \(A\) are nonnegative, then \(A\) is positive semidefinite.
		
		Since \(A\) is an \(n \times n\) symmetric matrix.\\
		\(A\) can be diagonalized by an orthogonal matrix \(P\):
		$$A = P \Lambda P^{-1}$$
		where \(\Lambda\) is a diagonal matrix containing the eigenvalues of \(A\) and \(P\) is an orthogonal matrix \(P^{-1} =P^{T}\).\\
		\(\lambda_1, \lambda_2, \dots, \lambda_n\) are the eigenvalues of \(A\),and \(\lambda_i \ge 0\) for \(1 \le i \le n \)\\
		For any nonzero vector \(\mathbf{x} \in \mathbb{R}^n\), we have:
		\[
		\mathbf{x}^T A \mathbf{x} = \mathbf{x}^T P \Lambda P^{-1} \mathbf{x} = \mathbf{x}^T P \Lambda P^{T} \mathbf{x}
		\]
		
		we set \(\mathbf{y} = P^T \mathbf{x}\).
		\[
		\mathbf{x}^T P \Lambda P^{T} \mathbf{x} = \mathbf{y}^T \Lambda \mathbf{y} = \sum_{i=1}^n \lambda_i y_{i}^2 
		\]
		Since \(\lambda_i \ge 0\) for \(1 \le i \le n \)
		\[\therefore \mathbf{x}^T A \mathbf{x} = \sum_{i=1}^n \lambda_i y_{i}^2 \ge 0\]
		Therefore, \(A\) is positive semidefinite.
		\end{enumerate}	
	\item Positive Definite Condition:\\
		If we replace \(\ge 0\) with \(> 0\) in the above proof, we can get the proof of the positive definite condition.\\
\end{enumerate}	
\end{PROOF}

\begin{excercise}\label{e4}
Suppose that $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex and upper bounded. Show that $f$ must
be a constant function.
\end{excercise}

\begin{PROOF}{e4}Proof:\\
Since \(f\) is upper bounded, there exists a constant \(M\) such that \\
\[f(\xB) \le M \ for\  all \ \xB \in \mathbb{R}^n\].

Since \(f\) is convex ,\(\forall \xB, \yB \in \mathbb{R}^n \)\\
	\[
		f(\lambda \xB+(1-\lambda) \yB)\le \lambda f(\xB)+(1-\lambda)f(\yB)
	\]
We assume $f$ is not a constant function, then there exists \(\xB, \yB \in \mathbb{R}^n\) such that \(f(\xB) \neq f(\yB)\).\\
To simplify the proof,we assume \(f(\xB) > f(\yB)\),we set $\xB = \frac{\xB-(1-\lambda)\yB}{\lambda}$,so we get:
	\[
	f(\xB) \le \lambda f(\frac{\xB-(1-\lambda)\yB}{\lambda}) + (1-\lambda)f(\yB)
	\]

	\[
	\therefore f(\frac{\xB-(1-\lambda)\yB}{\lambda}) \ge \frac{f(\xB)-f(\yB)}{\lambda}+f(\yB)
	\]

	\[
	\because \lim_{\lambda \to 0^{+}} \frac{f(\xB)-f(\yB)}{\lambda}+f(\yB) = +\infty
	\]

	\[
	\therefore f(\frac{\xB-(1-\lambda)\yB}{\lambda}) \ge +\infty > M
	\]
This is a contradiction to the assumption that \(f\) is upper bounded.\\
So, \(f\) must be a constant function.

\end{PROOF}

\end{document}